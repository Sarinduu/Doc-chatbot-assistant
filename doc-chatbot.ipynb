{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4c8bb52-4ba8-4c8b-835f-69ec55ce42cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Section 1: Imports\n",
    "\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import fitz  # PyMuPDF\n",
    "import faiss\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "import google.genai as genai\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f41ad184-83b8-4ab3-b06f-df4e1e199809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables Loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Section 2: Load environment variables\n",
    "\n",
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "CONTENT_FOLDER_PATH = os.getenv(\"CONTENT_FOLDER_PATH\")\n",
    "DOCUMENT_TITLE = os.getenv(\"DOCUMENT_TITLE\", \"Uploaded Documents\")\n",
    "\n",
    "print(\"Environment variables Loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e48e4c9b-6914-43c7-801f-fd30293c037b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded text from 1 document(s) in 'source_materials'\n"
     ]
    }
   ],
   "source": [
    "# Section 3: Load text content from multiple PDF and TXT files in a folder\n",
    "\n",
    "def load_documents(folder_path):\n",
    "    if not folder_path:\n",
    "        raise ValueError(\"Folder path is not set.\")\n",
    "\n",
    "    all_text = \"\"\n",
    "    pdf_files = glob.glob(os.path.join(folder_path, \"*.pdf\"))\n",
    "    txt_files = glob.glob(os.path.join(folder_path, \"*.txt\"))\n",
    "    total_files = 0\n",
    "\n",
    "    # Load PDFs\n",
    "    for pdf_file in pdf_files:\n",
    "        try:\n",
    "            doc = fitz.open(pdf_file)\n",
    "            for page in doc:\n",
    "                all_text += page.get_text()\n",
    "            total_files += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading PDF {pdf_file}: {e}\")\n",
    "\n",
    "    # Load TXTs\n",
    "    for txt_file in txt_files:\n",
    "        try:\n",
    "            with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                all_text += f.read() + \"\\n\"\n",
    "            total_files += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading TXT {txt_file}: {e}\")\n",
    "\n",
    "    if total_files == 0:\n",
    "        raise FileNotFoundError(f\"No PDF or TXT files found in folder: {folder_path}\")\n",
    "\n",
    "    print(f\"Loaded text from {total_files} document(s) in '{folder_path}'\")\n",
    "    return all_text\n",
    "\n",
    "# Load and confirm success\n",
    "notes_text = load_documents(CONTENT_FOLDER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "469451d9-1c8f-4051-bfe1-fbf65a574007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4: Split text into chunks\n",
    "\n",
    "# Recommended defaults for good context balance with Gemini (chunk_size=400, overlap=100)\n",
    "def split_text(text, chunk_size=400, overlap=100):\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        current_chunk.append(sentence.strip())\n",
    "        word_count = len(\" \".join(current_chunk).split())\n",
    "\n",
    "        if word_count >= chunk_size:\n",
    "            chunk = \" \".join(current_chunk).strip()\n",
    "            chunks.append(chunk)\n",
    "\n",
    "            if overlap > 0:\n",
    "                overlap_words = chunk.split()[-overlap:]\n",
    "                current_chunk = [\" \".join(overlap_words)]\n",
    "            else:\n",
    "                current_chunk = []\n",
    "\n",
    "    if current_chunk:\n",
    "        leftover = \" \".join(current_chunk).strip()\n",
    "        if len(leftover.split()) > overlap:\n",
    "            chunks.append(leftover)\n",
    "            \n",
    "    # For debugging: Show total number of chunks and preview first few for inspection\n",
    "    # print(f\"Total Chunks Created: {len(chunks)}\")\n",
    "    # for i, ch in enumerate(chunks[:3]):\n",
    "    #     print(f\"Chunk {i+1} Preview: {ch[:200]}...\\n\")\n",
    "\n",
    "    return chunks\n",
    "\n",
    "chunks = split_text(notes_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9f79720-0c8f-4176-8308-4ecdf16777c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22d6c588d7f74227b62623f914055057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generated and added to FAISS index (total: 5)\n"
     ]
    }
   ],
   "source": [
    "# Section 5: Generate embeddings and set up FAISS index\n",
    "\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')  # Fast & lightweight embedding model\n",
    "embeddings = embed_model.encode(chunks, show_progress_bar=True)\n",
    "dimension = embeddings.shape[1]\n",
    "\n",
    "# Use HNSW index (M=32: controls the number of graph links per node)\n",
    "index = faiss.IndexHNSWFlat(dimension, 32)\n",
    "index.hnsw.efConstruction = 200\n",
    "index.add(np.array(embeddings))\n",
    "\n",
    "print(f\"Embeddings generated and added to FAISS index (total: {len(embeddings)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64f6d000-3df3-48b3-81c5-55e65afb91ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 6: Define Search Function\n",
    "\n",
    "def search_notes(question, top_k=5, max_distance_threshold=0.3, enable_fallback=True):\n",
    "    \"\"\"\n",
    "    Retrieves the most relevant chunks based on vector similarity.\n",
    "\n",
    "    Args:\n",
    "        question (str): User question.\n",
    "        top_k (int): Number of chunks to retrieve.\n",
    "        max_distance_threshold (float): Maximum distance (smaller = more similar).\n",
    "        enable_fallback (bool): If True, returns the top result even if below threshold.\n",
    "\n",
    "    Returns:\n",
    "        list of (chunk, distance) tuples: Filtered chunks most relevant to the question.\n",
    "    \"\"\"\n",
    "    question_vec = embed_model.encode([question])\n",
    "    distances, indices = index.search(np.array(question_vec), top_k)\n",
    "\n",
    "    results = [\n",
    "        (chunks[i], dist)\n",
    "        for i, dist in zip(indices[0], distances[0])\n",
    "        if i < len(chunks) and dist < max_distance_threshold\n",
    "    ]\n",
    "\n",
    "    if enable_fallback and not results and indices[0][0] < len(chunks):\n",
    "        fallback_index = indices[0][0]\n",
    "        results.append((chunks[fallback_index], distances[0][0]))\n",
    "\n",
    "    #For debugging: print number of matched chunks and top match distance\n",
    "    # print(f\"Retrieved {len(results)} chunk(s) for question: \\\"{question}\\\"\")\n",
    "    # if results:\n",
    "    #     print(f\"Top match distance: {results[0][1]:.4f}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69fa3de2-6e96-4aed-915f-71a0cdccf6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 7: Connect to Gemini model\n",
    "\n",
    "if not GEMINI_API_KEY:\n",
    "    raise ValueError(\"GEMINI_API_KEY is not set in the .env file\")\n",
    "\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c3b0bf2-5ab1-4de4-8ca9-ed93b119755c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 8: Define Answer Generation Function\n",
    "\n",
    "def generate_answer(question, top_k=5):\n",
    "    context_chunks = search_notes(question, top_k=top_k)\n",
    "    context = \"\\n\".join([chunk for chunk, _ in context_chunks])\n",
    "\n",
    "    # Limit context to avoid token overflow\n",
    "    if len(context.split()) > 400:\n",
    "        context = \" \".join(context.split()[:400])\n",
    "\n",
    "    prompt = f\"\"\"You are a helpful, knowledgeable, and concise assistant. Answer the following question using only the information provided in the context below.\n",
    "\n",
    "Use complete sentences or point form as appropriate. If the answer is not found in the context, respond with:\n",
    "\"I'm sorry, but the information needed to answer this question is not available in the provided documents.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            contents=[prompt]\n",
    "        )\n",
    "        answer = response.text.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Gemini API error: {e}\"\n",
    "\n",
    "    def is_fallback_answer(text):\n",
    "        if not text or text.strip() == \"\":\n",
    "            return True\n",
    "        normalized = text.lower().strip()\n",
    "        fallback_phrases = [\n",
    "            \"i cannot answer\", \"not available\", \"not enough information\",\n",
    "            \"i don't know\", \"do not know\", \"not covered\"\n",
    "        ]\n",
    "        return text.startswith(\".\") or text.count(\".\") > 10 or any(phrase in normalized for phrase in fallback_phrases)\n",
    "\n",
    "    if is_fallback_answer(answer):\n",
    "        return \"Sorry, I couldn't find an answer based on the available content.\"\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb90b0f3-2a4f-429e-a25b-2a1acb24afed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Section 9: Set Up Gradio Chatbot UI\n",
    "\n",
    "with gr.Blocks(title=f\"Q&A Assistant for {DOCUMENT_TITLE}\") as demo:\n",
    "    gr.Markdown(f\"## Q&A Assistant for {DOCUMENT_TITLE}\")\n",
    "    gr.Markdown(f\"Ask any question related to your {DOCUMENT_TITLE.lower()}. The assistant will only use the provided content to generate answers.\")\n",
    "\n",
    "    with gr.Row():\n",
    "        question_input = gr.Textbox(\n",
    "            label=\"Your Question\",\n",
    "            placeholder=f\"Ask a question based on your {DOCUMENT_TITLE.lower()}...\",\n",
    "            lines=3,\n",
    "            scale=3\n",
    "        )\n",
    "        submit_btn = gr.Button(\"Get Answer\", scale=1)\n",
    "\n",
    "    with gr.Row():\n",
    "        answer_output = gr.Textbox(\n",
    "            label=\"Answer\",\n",
    "            lines=8,\n",
    "            max_lines=12,\n",
    "            interactive=False,\n",
    "            show_copy_button=True\n",
    "        )\n",
    "\n",
    "    def chatbot_interface(question):\n",
    "        if not question.strip():\n",
    "            return \"Please enter a valid question.\"\n",
    "\n",
    "        answer = generate_answer(question)\n",
    "\n",
    "        with open(\"answer_log.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"Q: {question}\\nA: {answer}\\n{datetime.now()}\\n\\n\")\n",
    "\n",
    "        return answer\n",
    "\n",
    "    submit_btn.click(\n",
    "        fn=chatbot_interface,\n",
    "        inputs=question_input,\n",
    "        outputs=answer_output,\n",
    "        show_progress=True\n",
    "    )\n",
    "\n",
    "    demo.launch(share=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a3ac37-41e8-4361-84db-73596a33bb3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ChatBot ENV",
   "language": "python",
   "name": "chatbot-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
